{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pptx import Presentation\n",
    "import nltk, collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "master_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the words in a ppt while retaining slide numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = str(input('Enter your filepath (or filename if in the same folder): ')) #filepath\n",
    "pres = Presentation(filepath)\n",
    "slideCount = 0\n",
    "indexes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slide in pres.slides:\n",
    "    slideCount += 1\n",
    "    words_in_slide = [slideCount]\n",
    "#     print('slide number', slideCount)\n",
    "    for shape in slide.shapes:\n",
    "        if (shape.has_text_frame):\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                for run in paragraph.runs:\n",
    "                    # print(run.text)\n",
    "                    # print(\"    ---- New Line ----    \")\n",
    "                    words_in_slide.append(run.text)\n",
    "    indexes[slideCount] = words_in_slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indexes)):\n",
    "    print(i+1, \" - - - \", indexes[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each slide's words, get the most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_terms = {'\\t','(',')','=','.','-',':'}\n",
    "numbers = set([i for i in '0123456789'])\n",
    "# alphabets = set([i for i in 'abcdefghijklmnopqrstuvwx'])\n",
    "\n",
    "def remove_all_instances(main_text, old_term, replacement=\"\"):\n",
    "    s = main_text.replace(old_term, replacement)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we're only using 1 slide but once we have it down, we can put it into a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT RUN\n",
    "\n",
    "slideNumber = 5\n",
    "slide = indexes[slideNumber]\n",
    "slideText = \" \".join(slide[1:]).strip()\n",
    "\n",
    "for term in unwanted_terms:\n",
    "    slideText = remove_all_instances(slideText,term)\n",
    "\n",
    "slideText\n",
    "# slideText_without_unwanted = slideText.replace(\"\\t\",\"\")\n",
    "# slideText_without_unwanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT RUN\n",
    "\n",
    "word_tokens = nltk.word_tokenize(slideText)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT RUN\n",
    "\n",
    "words_without_stops = [w for w in word_tokens if w not in stop_words]\n",
    "# words_without_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_with_n_characters(tokenised_list, n_characters = 1):\n",
    "    new_list = tokenised_list.copy()\n",
    "    for word in tokenised_list:\n",
    "        if (len(word) == n_characters):\n",
    "#             print(word, \" --- \", len(word))\n",
    "#             print(len(word))\n",
    "#             print(word)\n",
    "            new_list.remove(word)\n",
    "#             print(words_without_stops)\n",
    "#         else:\n",
    "#             print(\"NOT REJECTED: \",word, \" --- \", len(word))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT RUN\n",
    "\n",
    "words_without_singularsStops = remove_words_with_n_characters()\n",
    "\n",
    "words_without_singularsStops = [x.lower() for x in words_without_singularsStops]\n",
    "\n",
    "print(words_without_singularsStops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_without_singularsStops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7c516f90d41a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_without_singularsStops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words_without_singularsStops' is not defined"
     ]
    }
   ],
   "source": [
    "#DONT NEED TO RUN until end maybe\n",
    "\n",
    "frequencies = collections.Counter() \n",
    "for w in words_without_singularsStops:\n",
    "    frequencies[w] += 1\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT NEED TO RUN until end maybe\n",
    "\n",
    "bigram = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words_without_singularsStops)\n",
    "finder.apply_freq_filter(2)\n",
    "print (finder.nbest(bigram.pmi, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put into Index by page\n",
    "\n",
    "DefaultDict is useful in this case cause it creates a new list when the key is missing. It can be an issue if you're intending to use this as read_only, cause it will return an empty list instead of None. But since, we're not calling any values, it's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT RUN\n",
    "\n",
    "for word in set(words_without_singularsStops):\n",
    "    master_index[word].append(slideNumber)\n",
    "#     print(word, type(word))\n",
    "    \n",
    "print(master_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all of that into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_slide(slideNumber):\n",
    "    slide = indexes[slideNumber]\n",
    "    slideText = \" \".join(slide[1:]).strip()\n",
    "\n",
    "    for term in unwanted_terms:\n",
    "        slideText = remove_all_instances(slideText,term)\n",
    "    \n",
    "    for number in numbers:\n",
    "        slideText = remove_all_instances(slideText,number)\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(slideText)\n",
    "    \n",
    "    words_without_stops = [w for w in word_tokens if w not in stop_words]\n",
    "    \n",
    "    words_without_singularsStops = remove_words_with_n_characters(words_without_stops)\n",
    "\n",
    "    words_without_singularsStops = [x.lower() for x in words_without_singularsStops]\n",
    "    \n",
    "    for word in set(words_without_singularsStops):\n",
    "        master_index[word].append(slideNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indexes)):\n",
    "#     print(i+1, \" - - - \", indexes[i+1])\n",
    "    index_slide(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2â„“, found on pages [69] was not encoded\n",
      "2ğ´, found on pages [69] was not encoded\n",
      "2ğ‘˜ğ‘‡, found on pages [69] was not encoded\n",
      "2ğœ‹ğ‘˜ğ‘‡, found on pages [69] was not encoded\n",
      "3ğœ‹, found on pages [69] was not encoded\n",
      "3ğœ‹ğœ‡ğ·, found on pages [69] was not encoded\n",
      "4Î±Î·, found on pages [183] was not encoded\n",
      "eğ‘˜, found on pages [162, 162] was not encoded\n",
      "Î±Î·, found on pages [183] was not encoded\n",
      "Î¼m, found on pages [140, 140] was not encoded\n",
      "Ï€d, found on pages [183, 183] was not encoded\n",
      "Ï€ğ·, found on pages [91, 91] was not encoded\n",
      "Ïg, found on pages [47, 49, 50, 47, 49, 50] was not encoded\n",
      "Ïu, found on pages [47, 49, 50, 47, 49, 50] was not encoded\n",
      "âˆ‡ğ·âˆ‡ğ‘›, found on pages [104, 104] was not encoded\n",
      "âˆ‡ğ‘›, found on pages [147, 147] was not encoded\n",
      "âˆ’4ğ›¼ğ¿ğœ‚, found on pages [184] was not encoded\n",
      "âˆ’âˆ‡, found on pages [104, 104] was not encoded\n",
      "âˆ’ğ›¼ğ¿ğœ‚, found on pages [184] was not encoded\n",
      "âˆ’ğœ‡ğ‘š, found on pages [17, 19, 29, 32, 33, 17, 19, 29, 32, 33] was not encoded\n",
      "ï€ ï€‰ï€­, found on pages [132, 132] was not encoded\n",
      "ï€­ï€±, found on pages [114, 114] was not encoded\n",
      "ï€¾ï€¾, found on pages [157, 157] was not encoded\n",
      "ï°ï¡, found on pages [132, 132] was not encoded\n",
      "ï°ï­, found on pages [71, 71] was not encoded\n",
      "ğ¶â„ğ‘ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘ ğ‘¡ğ‘–ğ‘, found on pages [203, 204, 203, 204] was not encoded\n",
      "ğ·âˆ‡, found on pages [104, 147, 104, 147] was not encoded\n",
      "ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’, found on pages [203, 204, 203, 204] was not encoded\n",
      "ğ·ğ›», found on pages [147, 147] was not encoded\n",
      "ğ¾ğ‘›ğ´, found on pages [69, 69] was not encoded\n",
      "ğ¿ğ‘’ğ‘›ğ‘”ğ‘¡â„, found on pages [203, 204, 203, 204] was not encoded\n",
      "ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ğ‘’, found on pages [203, 204, 203, 204] was not encoded\n",
      "ğ‘†ğ‘¡, found on pages [203, 203] was not encoded\n",
      "ğ‘†ğ‘¡ğ‘œğ‘, found on pages [203, 204, 203, 204] was not encoded\n",
      "ğ‘ğ‘£, found on pages [162, 162] was not encoded\n",
      "ğ‘ğ‘š, found on pages [17, 19, 21, 29, 32, 33, 17, 19, 21, 29, 32, 33] was not encoded\n",
      "ğ‘‘ğ·, found on pages [21, 21] was not encoded\n",
      "ğ‘‘ğ‘, found on pages [23, 23] was not encoded\n",
      "ğ‘‘ğ‘›, found on pages [181, 181] was not encoded\n",
      "ğ‘‘ğ‘¥, found on pages [181, 181] was not encoded\n",
      "ğ‘‘ğ‘¥ğ‘‘ğ‘§, found on pages [181, 181] was not encoded\n",
      "ğ‘‘ğ‘¦, found on pages [181, 181] was not encoded\n",
      "ğ‘˜ğ‘‡, found on pages [69, 69] was not encoded\n",
      "ğ‘˜ğ‘‡ğ‘˜ğ‘‡, found on pages [64, 64] was not encoded\n",
      "ğ‘›ğ‘š, found on pages [21, 21] was not encoded\n",
      "ğ‘›ğœ•, found on pages [104, 104] was not encoded\n",
      "ğ‘ğ‘, found on pages [21, 21] was not encoded\n",
      "ğ›»ğ‘›, found on pages [147, 147] was not encoded\n",
      "ğ›¼ğ‘‘ğ‘§, found on pages [181, 181] was not encoded\n",
      "ğœ‹ğ·, found on pages [92, 92] was not encoded\n",
      "ğœ‹ğ‘‘, found on pages [184, 184] was not encoded\n",
      "ğœ‹ğ‘˜ğ‘‡, found on pages [69] was not encoded\n",
      "ğœ‹ğ‘š, found on pages [69, 69] was not encoded\n",
      "ğœ‹ğœ‡ğ·, found on pages [69] was not encoded\n",
      "ğœŒg, found on pages [91, 91] was not encoded\n",
      "ğœ•n, found on pages [147, 147] was not encoded\n",
      "ğœ•t, found on pages [147, 147] was not encoded\n",
      "ğœ•ğ‘›, found on pages [104, 147, 104, 147] was not encoded\n",
      "ğœ•ğ‘¡, found on pages [104, 147, 104, 147] was not encoded\n"
     ]
    }
   ],
   "source": [
    "output_str=\"\"\n",
    "with open('index.txt','w') as f:    \n",
    "        for elem in sorted(master_index):\n",
    "            try:\n",
    "                f.write(str(elem) + \" :: \" + str(master_index[elem]) + \"\\n\")\n",
    "            except UnicodeEncodeError:\n",
    "                print(f'{elem}, found on pages {master_index[elem]} was not encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}